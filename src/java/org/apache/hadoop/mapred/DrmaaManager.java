package org.apache.hadoop.mapred;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.util.Shell;

import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.*;

class DrmaaManager {
  public static final Log LOG = LogFactory.getLog(DrmaaManager.class);
  public static String newLineChar = System.getProperty("line.separator");
  public static SimpleDateFormat DATEFORMAT_ISO8601 = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ssZ");

  //Mapping from the JVM IDs to running Tasks
  //Map <TaskAttemptID,TaskRunner> taskAttemptToRunningTask = new HashMap<TaskAttemptID, TaskRunner>();
  //Map <TorqueJobID, TaskAttemptID> torqueTaskToTaskAttempt = new HashMap<TorqueJobID, TaskAttemptID>();
  TaskTracker taskTracker;

  public DrmaaManager(TaskTracker tracker) {
    this.taskTracker = tracker;    
  }

  public static void SubmitJob(
          Task task,
          List<String> setup,
          List<String> cmd,
          File workDir,
          Map<String, String> env,
          File stdout,
          File stderr
  ) throws IOException {

    LOG.info("TRQ> Submitting job in working directory = " + workDir.toString());
    String gfarmConfig = System.getenv("GFARM_CONFIG_FILE");
    if (gfarmConfig != null) env.put("GFARM_CONFIG_FILE", gfarmConfig.toString());

    File jobScriptFile = new File(workDir, "job.sh");

    StringBuffer command = new StringBuffer();
    command.append("exec ");
    command.append(TaskLog.addCommand(cmd, true));
    // TaskInProgress tip = taskTracker.task.get()

    // TODO: change permissions for the job.sh


    String taskId = task.getTaskID().toString();
    // wirte the commands for the job
    BufferedWriter jobScript = new BufferedWriter(new FileWriter(jobScriptFile));
    jobScript.write("#!/bin/sh");
    jobScript.write(newLineChar);
    jobScript.write("# DATE: " + DATEFORMAT_ISO8601.format(new Date()));
    jobScript.write(newLineChar);
    jobScript.write("# HADOOP Job ID: " + task.getJobID());
    jobScript.write(newLineChar);
    jobScript.write("# HADOOP Task Attempt ID: " + taskId);
    jobScript.write(newLineChar);
    jobScript.write("#");
    jobScript.write("#");
    jobScript.write(newLineChar);
    jobScript.write(newLineChar);
    jobScript.write("# This file was automatically generated by the G-Hadoop framework.");
    jobScript.write(newLineChar);
    jobScript.write("# set working directory");
    jobScript.write(newLineChar);
    jobScript.write("#PBS -q " + (task.isMapTask() ? "mappers" : "reducers"));
    jobScript.write(newLineChar);
    //jobScript.write("#PBS -l nodes=GH1-TRQ-01");
    jobScript.write(newLineChar);
    jobScript.write("#PBS -d " + workDir.getAbsolutePath());
    jobScript.write(newLineChar);
    jobScript.write("#PBS -e " + stderr.getAbsolutePath());
    jobScript.write(newLineChar);
    jobScript.write("#PBS -o " + stdout.getAbsolutePath());
    jobScript.write(newLineChar);
    jobScript.write("#PBS -l nodes=1");
    jobScript.write(newLineChar);

    String typeFlag = task.isMapTask() ? "M" : "R";
    if (task.isJobSetupTask()) typeFlag = "S";
    if (task.isJobCleanupTask()) typeFlag = "C";
    if (task.isTaskCleanupTask()) typeFlag = "c";

    String jobName =  typeFlag + ":" + task.conf.getJobName().replaceAll("\\W","_");
    jobScript.write("#PBS -N " + jobName); // +  jobName.substring(0, Math.min(jobName.length() - 1, 14)));
    jobScript.write(newLineChar);
    jobScript.write("#");
    jobScript.write("# Setup the environment");
    jobScript.write(newLineChar);
    for (Map.Entry<String,String> envVar : env.entrySet()) {
      jobScript.write("export ");
      jobScript.write(envVar.getKey());
      jobScript.write("=");
      jobScript.write(envVar.getValue());
      jobScript.write(newLineChar);
    }
    jobScript.write("#");
    jobScript.write(newLineChar);
    jobScript.write(command.toString());
    jobScript.close();

    // TODO: read the job id from the stdout

    List<String> shellCommand = new ArrayList<String>(3);
    shellCommand.add("qsub");
    shellCommand.add(jobScriptFile.toString());

    LOG.info("executing command: " + shellCommand.toArray().toString());
    Shell.ShellCommandExecutor shexec = new Shell.ShellCommandExecutor(shellCommand.toArray(new String[0]), workDir, env);
    shexec.execute();
  }

  synchronized public void stop() {
    // TODO: kill all running jobs here, since the task tracker is going down
  }

  public void taskFinished(DrmaaJobID id) {
    // TODO: task has reported that it is finished
  }

  public class DrmaaJobID extends ID {

  }
}
